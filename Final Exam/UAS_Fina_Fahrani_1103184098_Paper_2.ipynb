{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FINAL EXAM_Fina Fahrani_1103184098_Paper 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1HLB60EHsnHxHjTYOjJG8pIzBoHZUAPTs",
      "authorship_tag": "ABX9TyOle2scxBVsHBafW5sV6TIF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FinaFahrani/Kumpulan-Tugas_-Machine-Learning/blob/main/Final%20Exam/UAS_Fina_Fahrani_1103184098_Paper_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reproduce code : https://github.com/Vedurumudi-Priyanka/Twitter-Sentiment-Analysis\n"
      ],
      "metadata": {
        "id": "JtNpjNqAEvqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install twython # untuk API Twitter resmi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9yC9a3FyRiF",
        "outputId": "04abbbac-bb57-4700-f7a3-ce0fb28c7d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting twython\n",
            "  Downloading twython-3.9.1-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from twython) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from twython) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.1.1)\n",
            "Installing collected packages: twython\n",
            "Successfully installed twython-3.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utils**"
      ],
      "metadata": {
        "id": "jzUPAyLJ3BJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys #mengakses konfigurasi interpreter\n",
        "import pickle #untuk menyimpan dan membaca data ke dalam /dari sebuah file\n",
        "import random #untuk  mengubah urutan setiap elemen yang ada pada list\n",
        "\n",
        "#membuat class utils yang akan digunakan pada saat training dataset\n",
        "def file_to_wordset(filename):#untuk converts file dari kata perbaris menjadi set python\n",
        "    words = []\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            words.append(line.strip())\n",
        "    return set(words)\n",
        "\n",
        "\n",
        "def write_status(i, total): #untuk melakukan penulisan status ke console\n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.write('Processing %d/%d' % (i, total))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def save_results_to_csv(results, csv_file): #untuk menyimpan hasil dalam bentuk csv dalam format kaggle\n",
        "    with open(csv_file, 'w') as csv:\n",
        "        csv.write('id,prediction\\n')\n",
        "        for tweet_id, pred in results:\n",
        "            csv.write(tweet_id)\n",
        "            csv.write(',')\n",
        "            csv.write(str(pred))\n",
        "            csv.write('\\n')\n",
        "\n",
        "\n",
        "def top_n_words(pkl_file_name, N, shift=0): #untuk mengambil kata teratas dari file\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
        "    return words\n",
        "\n",
        "\n",
        "def top_n_bigrams(pkl_file_name, N, shift=0): #untuk mengambil bigram teratas dari file yang memiliki objek Penghitung yang dihasilkan oleh stats.py\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    bigrams = {p[0]: i for i, p in enumerate(most_common)}\n",
        "    return bigrams\n",
        "\n",
        "\n",
        "def split_data(tweets, validation_split=0.1): #untuk split dataset menjadi training dan validation sets\n",
        "    index = int((1 - validation_split) * len(tweets))\n",
        "    random.shuffle(tweets)\n",
        "    return tweets[:index], tweets[index:]\n"
      ],
      "metadata": {
        "id": "a1clZ4IORchZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "IEnRdOi7T0e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mengambil alamat/path dataset train.csv lalu dilakukan preprocessed.py \n",
        "!python2 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/preprocess.py /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC9BBAo11xjN",
        "outputId": "0b3dec2a-cc37-432e-ba92-bee6990b5ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Saved processed tweets to: /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mengambil alamat/path dataset test.csv lalu dilakukan preprocessed.py \n",
        "#pada file preproceseed.py line 107 bagian test_file diubah menjadi True\n",
        "!python2 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/preprocess.py /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/test.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_BpzCtC58JI",
        "outputId": "3fb51422-a904-486b-bf3c-df87d4fd6027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 300000/300000\n",
            "Saved processed tweets to: /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stats**"
      ],
      "metadata": {
        "id": "futxtmyOgNYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mengecek dataset train-processed.csv yang telah di preprocessed untuk mengetahui berapa kata positive dan negative dalam dataset\n",
        "!python2 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/stats.py /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNpfKxW89opd",
        "outputId": "9f4a3728-a66c-42e2-8a6e-444c1c9ab9db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 100000, Positive: 56462, Negative: 43538\n",
            "User Mentions => Total: 88959, Avg: 0.8896, Max: 12\n",
            "URLs => Total: 3602, Avg: 0.0360, Max: 4\n",
            "Emojis => Total: 1001, Positive: 856, Negative: 145, Avg: 0.0100, Max: 5\n",
            "Words => Total: 1192424, Unique: 50376, Avg: 11.9242, Max: 40, Min: 0\n",
            "Bigrams => Total: 1092958, Unique: 385024, Avg: 10.9296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Baseline**\n"
      ],
      "metadata": {
        "id": "XxkrAN7SSA8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#klasifikasi tweet dari kata positif dan negatif menggunakan baseline\n",
        "\n",
        "#dataset yang akan digunakan\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "POSITIVE_WORDS_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/positive-words.txt'\n",
        "NEGATIVE_WORDS_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/positive-words.txt'\n",
        "TRAIN = True\n",
        "\n",
        "\n",
        "def classify(processed_csv, test_file=True, **params): #untuk melakukan klasifikasi data pada file yang sudah dilakukan preprocessed\n",
        "    positive_words = file_to_wordset(params.pop('positive_words'))\n",
        "    negative_words = file_to_wordset(params.pop('negative_words'))\n",
        "    predictions = []\n",
        "    with open(processed_csv, 'r') as csv: #membuka file preprocessed sebagai csv\n",
        "        for line in csv:\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.strip().split(',')\n",
        "            else:\n",
        "                tweet_id, label, tweet = line.strip().split(',')\n",
        "            pos_count, neg_count = 0, 0\n",
        "            for word in tweet.split():\n",
        "                if word in positive_words:\n",
        "                    pos_count += 1\n",
        "                elif word in negative_words:\n",
        "                    neg_count += 1\n",
        "            #print (pos_count, neg_count)\n",
        "            prediction = 1 if pos_count >= neg_count else 0 #untuk melakukan prediksi dataset \n",
        "            if test_file:\n",
        "                predictions.append((tweet_id, prediction))\n",
        "            else:\n",
        "                predictions.append((tweet_id, int(label), prediction))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "if __name__ == '__main__': #untuk menampilkan dan menyimpan hasil prediksi dataset \n",
        "    if TRAIN:\n",
        "        predictions = classify(TRAIN_PROCESSED_FILE, test_file=(not TRAIN), positive_words=POSITIVE_WORDS_FILE, negative_words=NEGATIVE_WORDS_FILE)\n",
        "        correct = sum([1 for p in predictions if p[1] == p[2]]) * 100.0 / len(predictions)\n",
        "        #menampilkan hasil akurasi training dataset\n",
        "        print ('Correct = %.2f%%' % correct)\n",
        "    else:\n",
        "        predictions = classify(TEST_PROCESSED_FILE, test_file=(not TRAIN), positive_words=POSITIVE_WORDS_FILE, negative_words=NEGATIVE_WORDS_FILE)\n",
        "        save_results_to_csv(predictions, '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/baseline.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM9w_Zl5RrrG",
        "outputId": "6d26716b-929b-4f3b-b5aa-fd7b7ca1e653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct = 56.46%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Naive Bayes**"
      ],
      "metadata": {
        "id": "4wS52T29UCYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random #untuk  mengubah urutan setiap elemen yang ada pada list\n",
        "import numpy as np #untuk melakukan komputasi numerik\n",
        "from sklearn.naive_bayes import MultinomialNB #untuk klasifikasi dengan fitur diskrit (misalnya, jumlah kata untuk klasifikasi teks).\n",
        "from scipy.sparse import lil_matrix #menangani sparse data dan  membuat matriks\n",
        "from sklearn.feature_extraction.text import TfidfTransformer #untuk pembobotan kata dalam klasifikasi dokumen.\n",
        "import pandas as pd #untuk  mengecek data\n",
        "# Performs classification using Naive Bayes.\n",
        "\n",
        "#dataset dan variabel yang dibutuhkan\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "xrange = range\n",
        "\n",
        "def get_feature_vector(tweet): #untuk mendapatkan vektor pada tweet\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'): #melakukan ekstrasi feature tweet\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X): #untuk mendapatkan nilai tf idf \n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True): #untuk mendapatkan vector feature\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = MultinomialNB()\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.partial_fit(training_set_X, training_set_y, classes=[0, 1])\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency': #melakukan prediksi dataset\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/naivebayes.csv')\n",
        "        print ('\\nSaved to naivebayes.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4vpbNLcV1Qr",
        "outputId": "b7d90117-c98f-4f16-fffd-5bb4156304c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7791/10000 = 77.9100 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/naivebayes.py"
      ],
      "metadata": {
        "id": "Zw7nRWFNUEi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89200c06-f2ed-42a3-8ef2-5b182b3c8884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7777/10000 = 77.7700 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Maximum Entropy**\n",
        "\n",
        "maxent-nltk.py\n"
      ],
      "metadata": {
        "id": "47nfGgwUUbhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk #set library untuk NLP\n",
        "import random  #untuk  mengubah urutan setiap elemen yang ada pada list\n",
        "import pickle #untuk menyimpan dan membaca data ke dalam /dari sebuah file\n",
        "import sys #mengakses konfigurasi interpreter\n",
        "import numpy as np #untuk melakukan komputasi numerik\n",
        "# Performs classification using Maximun Entropy.\n",
        "\n",
        "#dataset serta atribut yang diperlukan\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "USE_BIGRAMS = False\n",
        "TRAIN = True\n",
        "xrange = range\n",
        "\n",
        "def get_data_from_file(file_name, isTrain=True): #untuk mendapatkan data dari file yang akan di klasifikasi\n",
        "    data = []\n",
        "    with open(train_csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if isTrain:\n",
        "                tag = line.split(',')[1]\n",
        "                bag_of_words = line.split(',')[2].split()\n",
        "                if USE_BIGRAMS:\n",
        "                    bag_of_words_bigram = list(nltk.bigrams(line.split(',')[2].split()))\n",
        "                    bag_of_words = bag_of_words+bag_of_words_bigram\n",
        "            else :\n",
        "                tag = '5'\n",
        "                bag_of_words = line.split(',')[1].split()\n",
        "                if USE_BIGRAMS:\n",
        "                    bag_of_words_bigram = list(nltk.bigrams(line.split(',')[1].split()))\n",
        "                    bag_of_words = bag_of_words+bag_of_words_bigram\n",
        "            data.append((bag_of_words, tag))\n",
        "    return data\n",
        "\n",
        "def split_data(tweets, validation_split=0.1): #untuk validation/test split dataset\n",
        "    index = int((1 - validation_split) * len(tweets))\n",
        "    random.shuffle(tweets)\n",
        "    return tweets[:index], tweets[index:]\n",
        "\n",
        "def list_to_dict(words_list): #untuk membuat word list\n",
        "    return dict([(word, True) for word in words_list])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train = True\n",
        "    np.random.seed(1337)\n",
        "    train_csv_file = TRAIN_PROCESSED_FILE\n",
        "    test_csv_file = TEST_PROCESSED_FILE\n",
        "    train_data = get_data_from_file(train_csv_file, isTrain=True)\n",
        "    train_set, validation_set = split_data(train_data)\n",
        "    training_set_formatted = [(list_to_dict(element[0]), element[1]) for element in train_set]\n",
        "    validation_set_formatted = [(list_to_dict(element[0]), element[1]) for element in validation_set]\n",
        "    numIterations = 1\n",
        "    algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[1]\n",
        "    classifier = nltk.MaxentClassifier.train(training_set_formatted, algorithm, max_iter=numIterations)\n",
        "    classifier.show_most_informative_features(10)\n",
        "    count = int(0)\n",
        "    for review in validation_set_formatted: #untuk melakukasi prediksi akurasi\n",
        "        label = review[1]\n",
        "        text = review[0]\n",
        "        determined_label = classifier.classify(text)\n",
        "        #print(determined_label, label)\n",
        "        if determined_label!=label:\n",
        "            count+=int(1)\n",
        "    accuracy = (len(validation_set)-count)/len(validation_set)\n",
        "    print ('Validation set accuracy:%.4f'% (accuracy))\n",
        "    f = open('maxEnt_classifier.pickle', 'wb')\n",
        "    pickle.dump(classifier, f)\n",
        "    f.close()\n",
        "    print ('\\nPredicting for test data')\n",
        "    test_data = get_data_from_file(test_csv_file, isTrain=False)\n",
        "    test_set_formatted = [(list_to_dict(element[0]), element[1]) for element in test_data]\n",
        "    tweet_id = int(0)\n",
        "    results = []\n",
        "    for review in test_set_formatted:\n",
        "        text = review[0]\n",
        "        label = classifier.classify(text)\n",
        "        results.append((str(tweet_id), label))\n",
        "        tweet_id += int(1)\n",
        "    save_results_to_csv(results, '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/maxent.csv')\n",
        "    print ('\\nSaved to maxent.csv')"
      ],
      "metadata": {
        "id": "S4AT0OScaL1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "226778da-e8d5-4b95-cd32-fc46170a992f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ==> Training (1 iterations)\n",
            "\n",
            "      Iteration    Log Likelihood    Accuracy\n",
            "      ---------------------------------------\n",
            "             1          -0.69315        0.565\n",
            "         Final          -0.61640        0.756\n",
            "  -1.010 andyhurleyday==True and label is '0'\n",
            "   1.000 atthepool==True and label is '1'\n",
            "   1.000 princecharming==True and label is '0'\n",
            "   1.000 owiree==True and label is '0'\n",
            "   1.000 halfbloodprince==True and label is '1'\n",
            "   1.000 harpersglobe==True and label is '1'\n",
            "   1.000 hiccuuppss==True and label is '0'\n",
            "   1.000 thestreetforce==True and label is '1'\n",
            "   1.000 madnessness==True and label is '0'\n",
            "   1.000 hitech==True and label is '1'\n",
            "Validation set accuracy:0.7062\n",
            "\n",
            "Predicting for test data\n",
            "\n",
            "Saved to maxent.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Tree**"
      ],
      "metadata": {
        "id": "m7J_-WwZUpl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random #untuk  mengubah urutan setiap elemen yang ada pada list\n",
        "import numpy as np #untuk melakukan komputasi numerik\n",
        "from sklearn.tree import DecisionTreeClassifier #untuk menggunakan DecisionTreeClassifier dari sklearn\n",
        "from scipy.sparse import lil_matrix #menangani sparse data dan  membuat matriks\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Performs classification using Decision Tree.\n",
        "#dataset untuk klasifikasi decision tree\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "\n",
        "# True while training.\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "\n",
        "# If using bigrams.\n",
        "USE_BIGRAMS = False\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "xrange = range\n",
        "\n",
        "def get_feature_vector(tweet): #untuk mendapatkan vektor pada tweet\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'): #melakukan ekstrasi feature tweet\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    \"\"\"\n",
        "    Fits X for TF-IDF vectorization and returns the transformer.\n",
        "    \"\"\"\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams =top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = DecisionTreeClassifier(max_depth=25)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/decisiontree.csv')\n",
        "        print ('\\nSaved to decisiontree.csv')"
      ],
      "metadata": {
        "id": "O2LaG8xsUmiD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90baf72a-7819-4b3f-9a9d-bc89cbbd96bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 6853/10000 = 68.5300 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/decisiontree.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz3vHg8WZf2Q",
        "outputId": "11b026ae-1bd3-43fd-f497-33ddc763ca78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 6814/10000 = 68.1400 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest**"
      ],
      "metadata": {
        "id": "NxeHNW9nUzNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random #untuk  mengubah urutan setiap elemen yang ada pada list\n",
        "import numpy as np #untuk melakukan komputasi numerik\n",
        "from sklearn.ensemble import RandomForestClassifier #untuk menggunakan  metode random forest classifier \n",
        "from scipy.sparse import lil_matrix #menangani sparse data dan  membuat matriks\n",
        "from sklearn.feature_extraction.text import TfidfTransformer #untuk pembobotan kata dalam klasifikasi dokumen.\n",
        "\n",
        "# Performs classification using RandomForest classifier.\n",
        "#dataset dan variabel yang dibutuhkan\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = False\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'presence'\n",
        "xrange = range\n",
        "\n",
        "def get_feature_vector(tweet): #untuk mendapatkan vektor pada tweet\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'): #melakukan ekstrasi feature tweet\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = RandomForestClassifier(n_jobs=2, random_state=0)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/randomforest.csv')\n",
        "        print ('\\nSaved to randomforest.csv')"
      ],
      "metadata": {
        "id": "86BsFhwtU0T2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2baea22-31f2-491a-f588-e91c4f053daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7582/10000 = 75.8200 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/randomforest.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI8JwjznZ9Wj",
        "outputId": "8455184f-6c29-4dbc-8369-e52d4f9c27b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7174/10000 = 71.7400 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **XGBoost**"
      ],
      "metadata": {
        "id": "3v6ISfx3U-JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random #untuk  mengubah urutan setiap elemen yang ada pada list\n",
        "import numpy as np #untuk melakukan komputasi numerik\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import lil_matrix #menangani sparse data dan  membuat matriks\n",
        "from sklearn.feature_extraction.text import TfidfTransformer #untuk pembobotan kata dalam klasifikasi dokumen.\n",
        "\n",
        "# Performs classification using XGBoost.\n",
        "\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "xrange = range\n",
        "\n",
        "def get_feature_vector(tweet): #untuk mendapatkan vektor pada tweet\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'): #melakukan ekstrasi feature tweet\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets =split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = XGBClassifier(max_depth=25, silent=False, n_estimators=400)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/xgboost.csv')\n",
        "        print ('\\nSaved to xgboost.csv')"
      ],
      "metadata": {
        "id": "3v4GA1RoVAhV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d42b6a-3342-4c6b-b45d-2bfc2b2dc34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7630/10000 = 76.3000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM**"
      ],
      "metadata": {
        "id": "gqpxoADpXWLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm #untuk menggukan klasifikasi SVM\n",
        "import random #untuk  mengubah urutan setiap elemen yang ada pada list\n",
        "import numpy as np #untuk melakukan komputasi numerik\n",
        "from scipy.sparse import lil_matrix #menangani sparse data dan  membuat matriks\n",
        "from sklearn.feature_extraction.text import TfidfTransformer #untuk pembobotan kata dalam klasifikasi dokumen.\n",
        "\n",
        "# Performs classification using SVM.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "xrange = range\n",
        "\n",
        "def get_feature_vector(tweet): #untuk mendapatkan vektor pada tweet\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'): #melakukan ekstrasi feature tweet\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = svm.LinearSVC(C=0.1)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/svm.csv')\n",
        "        print ('\\nSaved to svm.csv')"
      ],
      "metadata": {
        "id": "6lRTYd9OXaSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7158ae2b-9ffd-4bbf-f9ba-11ad91f637b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7845/10000 = 78.4500 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/svm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGnDgIEGbIWB",
        "outputId": "4fdd0715-f30d-4a3b-d453-521e2b9e69dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7892/10000 = 78.9200 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi-Layer Perceptron**\n",
        "\n",
        "neuralnet.py"
      ],
      "metadata": {
        "id": "7J1ShhfxX-wH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "import sys #mengakses konfigurasi interpreter\n",
        "import random #untuk  mengubah urutan setiap elemen yang ada pada list\n",
        "import numpy as np #untuk melakukan komputasi numerik\n",
        "\n",
        "# Performs classification using an MLP/1-hidden-layer NN.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = False\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "xrange = range\n",
        "\n",
        "def get_feature_vector(tweet): #untuk mendapatkan vektor pada tweet\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'): #melakukan ekstrasi feature tweet\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = np.zeros((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(500, input_dim=VOCAB_SIZE, activation='sigmoid'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model, val_tweets):\n",
        "    correct, total = 0, len(val_tweets)\n",
        "    for val_set_X, val_set_y in extract_features(val_tweets, feat_type=FEAT_TYPE, test_file=False):\n",
        "        prediction = model.predict_on_batch(val_set_X)\n",
        "        prediction = np.round(prediction)\n",
        "        correct += np.sum(prediction == val_set_y[:, None])\n",
        "    return float(correct) / total\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    nb_epochs = 5\n",
        "    batch_size = 500\n",
        "    model = build_model()\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    best_val_acc = 0.0\n",
        "    for j in xrange(nb_epochs):\n",
        "        i = 1\n",
        "        for training_set_X, training_set_y in extract_features(train_tweets, feat_type=FEAT_TYPE, batch_size=batch_size, test_file=False):\n",
        "            o = model.train_on_batch(training_set_X, training_set_y)\n",
        "            sys.stdout.write('\\rIteration %d/%d, loss:%.4f, acc:%.4f' %\n",
        "                             (i, n_train_batches, o[0], o[1]))\n",
        "            sys.stdout.flush()\n",
        "            i += 1\n",
        "        val_acc = evaluate_model(model, val_tweets)\n",
        "        print ('\\nEpoch: %d, val_acc:%.4f' % (j + 1, val_acc))\n",
        "        random.shuffle(train_tweets)\n",
        "        if val_acc > best_val_acc:\n",
        "            print ('Accuracy improved from %.4f to %.4f, saving model' % (best_val_acc, val_acc))\n",
        "            best_val_acc = val_acc\n",
        "            model.save('best_model.h5')\n",
        "    print ('Testing')\n",
        "    del train_tweets\n",
        "    del model\n",
        "    model = load_model('best_model.h5')\n",
        "    test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "    n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "    predictions = np.array([])\n",
        "    print ('Predicting batches')\n",
        "    i = 1\n",
        "    for test_set_X, _ in extract_features(test_tweets, feat_type=FEAT_TYPE, batch_size=batch_size, test_file=True):\n",
        "        prediction = np.round(model.predict_on_batch(test_set_X).flatten())\n",
        "        predictions = np.concatenate((predictions, prediction))\n",
        "        write_status(i, n_test_batches)\n",
        "        i += 1\n",
        "    predictions = [(str(j), int(predictions[j]))\n",
        "                   for j in range(len(test_tweets))]\n",
        "    save_results_to_csv(predictions, '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/1layerneuralnet.csv')\n",
        "    print ('\\nSaved to 1layerneuralnet.csv')"
      ],
      "metadata": {
        "id": "clPcQNy7YA6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3821f1e6-ebe3-4484-db42-fc38651c2f90"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Iteration 180/180, loss:0.5645, acc:0.7080\n",
            "Epoch: 1, val_acc:0.7537\n",
            "Accuracy improved from 0.0000 to 0.7537, saving model\n",
            "Iteration 180/180, loss:0.4692, acc:0.7760\n",
            "Epoch: 2, val_acc:0.7621\n",
            "Accuracy improved from 0.7537 to 0.7621, saving model\n",
            "Iteration 180/180, loss:0.4520, acc:0.8180\n",
            "Epoch: 3, val_acc:0.7619\n",
            "Iteration 180/180, loss:0.4088, acc:0.8260\n",
            "Epoch: 4, val_acc:0.7570\n",
            "Iteration 180/180, loss:0.4361, acc:0.7880\n",
            "Epoch: 5, val_acc:0.7557\n",
            "Testing\n",
            "Generating feature vectors\n",
            "Processing 300000/300000\n",
            "\n",
            "Predicting batches\n",
            "Processing 600/600\n",
            "Saved to 1layerneuralnet.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/neuralnet.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EUAQFfOb2aY",
        "outputId": "71c977fc-2bb0-4cc5-bd19-c3054b248064"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "2022-01-24 09:02:09.334257: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Iteration 180/180, loss:0.4841, acc:0.7860\n",
            "Epoch: 1, val_acc:0.7613\n",
            "Accuracy improved from 0.0000 to 0.7613, saving model\n",
            "Iteration 180/180, loss:0.4915, acc:0.7680\n",
            "Epoch: 2, val_acc:0.7681\n",
            "Accuracy improved from 0.7613 to 0.7681, saving model\n",
            "Iteration 180/180, loss:0.4521, acc:0.7960\n",
            "Epoch: 3, val_acc:0.7628\n",
            "Iteration 180/180, loss:0.4391, acc:0.8020\n",
            "Epoch: 4, val_acc:0.7655\n",
            "Iteration 180/180, loss:0.4161, acc:0.7920\n",
            "Epoch: 5, val_acc:0.7649\n",
            "Testing\n",
            "Generating feature vectors\n",
            "Processing 300000/300000\n",
            "\n",
            "Predicting batches\n",
            "Processing 600/600\n",
            "Saved to 1layerneuralnet.csv\n"
          ]
        }
      ]
    }
  ]
}